# app.py
# ===========================================
# Streamlit-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:
# - –°–æ–±–∏—Ä–∞–µ—Ç –æ—Ç–∑—ã–≤—ã App Store (Apple RSS JSON) –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∞–º
# - –§–∏–ª—å—Ç—Ä—É–µ—Ç –ø–æ –¥–∞—Ç–µ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π)
# - –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–æ–ª–µ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã, –±–µ–∑ langdetect)
# - –î–µ–ª–∞–µ—Ç –∞–≤—Ç–æ-—Ç—ç–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–º (rule-based)
# - –î–∞—ë—Ç —Å–∫–∞—á–∞—Ç—å CSV –∏–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
#
# –ó–∞–ø—É—Å–∫:
#   pip install -r requirements.txt
#   streamlit run app.py
# ===========================================

import re
import time
import csv
import hashlib
import random
from datetime import datetime, timezone

import requests
import pandas as pd
import streamlit as st
from dateutil.relativedelta import relativedelta


# -----------------------------
# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ storefront/country –∫–æ–¥–æ–≤ (ISO 3166-1 alpha-2)
# -----------------------------
STORE_FRONTS = [
    "ae","ag","ai","al","am","ao","ar","at","au","az",
    "bb","be","bf","bg","bh","bj","bm","bn","bo","br","bs","bt","bw","by","bz",
    "ca","cg","ch","cl","cn","co","cr","cv","cy","cz",
    "de","dk","dm","do","dz",
    "ec","ee","eg","es",
    "fi","fj","fm","fr",
    "gb","gd","ge","gh","gm","gr","gt","gy",
    "hk","hn","hr","hu",
    "id","ie","il","in","iq","is","it",
    "jm","jo","jp",
    "ke","kg","kh","kn","kr","kw","ky","kz",
    "la","lb","lc","li","lk","lr","lt","lu","lv","ly",
    "ma","md","me","mg","mk","ml","mn","mo","mr","ms","mt","mu","mv","mw","mx","my","mz",
    "na","ne","ng","ni","nl","no","np","nz",
    "om",
    "pa","pe","pg","ph","pk","pl","pt","py",
    "qa",
    "ro","rs","ru","rw",
    "sa","sb","sc","se","sg","si","sk","sl","sn","sr","st","sv","sz",
    "tc","td","th","tj","tm","tn","tr","tt","tw","tz",
    "ua","ug","us","uy","uz",
    "vc","ve","vg","vn",
    "za",
]


# -----------------------------
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ Streamlit
# -----------------------------
def ui_log(log_box: st.delta_generator.DeltaGenerator, msg: str):
    ts = datetime.now().strftime("%H:%M:%S")
    log_box.write(f"[{ts}] {msg}")


# -----------------------------
# –ù–∞–¥—ë–∂–Ω—ã–µ HTTP-–∑–∞–ø—Ä–æ—Å—ã: retry + —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞
# -----------------------------
def request_with_retry(
    session: requests.Session,
    url: str,
    params: dict | None = None,
    timeout: int = 25,
    max_retries: int = 6,
    base_sleep: float = 0.75,
    jitter: float = 0.25,
):
    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø—Ä–∏ 429/5xx –∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–∫–∞—Ö
    for attempt in range(max_retries):
        try:
            r = session.get(url, params=params, timeout=timeout)
            status = r.status_code

            if status == 200:
                return r

            if status in (429, 500, 502, 503, 504):
                sleep_s = base_sleep * (2 ** attempt) + random.random() * jitter
                time.sleep(sleep_s)
                continue

            return None

        except requests.RequestException:
            sleep_s = base_sleep * (2 ** attempt) + random.random() * jitter
            time.sleep(sleep_s)

    return None


# -----------------------------
# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ app_id –∏ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω—ã –∏–∑ URL
# -----------------------------
def extract_app_id(app_url: str) -> str:
    m = re.search(r"/id(\d+)", app_url)
    if not m:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å app_id –∏–∑ URL. –û–∂–∏–¥–∞–µ—Ç—Å—è /idXXXXXXXXX –≤ —Å—Å—ã–ª–∫–µ.")
    return m.group(1)

def extract_default_country_from_url(app_url: str) -> str:
    m = re.search(r"apps\.apple\.com/([a-z]{2})/", app_url.lower())
    return m.group(1) if m else "us"


# -----------------------------
# iTunes Lookup: –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ —Å—Ç—Ä–∞–Ω–µ + app_name
# -----------------------------
def itunes_lookup(session: requests.Session, app_id: str, country: str) -> dict | None:
    url = "https://itunes.apple.com/lookup"
    r = request_with_retry(session, url, params={"id": app_id, "country": country})
    if not r:
        return None
    try:
        data = r.json()
    except Exception:
        return None
    if data.get("resultCount", 0) < 1:
        return None
    return data

def get_app_name(session: requests.Session, app_id: str, preferred_country: str) -> str | None:
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ç—Ä–∞–Ω—É –∏–∑ URL, –ø–æ—Ç–æ–º US –∫–∞–∫ fallback
    for c in [preferred_country, "us"]:
        data = itunes_lookup(session, app_id, c)
        if data and data.get("results"):
            return data["results"][0].get("trackName")
    return None


# -----------------------------
# Apple RSS JSON endpoint: customerreviews
# -----------------------------
def build_rss_url(country: str, app_id: str, page: int) -> str:
    return f"https://itunes.apple.com/{country}/rss/customerreviews/page={page}/id={app_id}/sortby=mostrecent/json"

def parse_rss_reviews(feed_json: dict) -> list[dict]:
    # –†–∞–∑–±–∏—Ä–∞–µ–º JSON –≤ —Å–ø–∏—Å–æ–∫ –æ—Ç–∑—ã–≤–æ–≤
    feed = (feed_json or {}).get("feed", {})
    entries = feed.get("entry", [])
    if isinstance(entries, dict):
        entries = [entries]

    parsed = []
    for e in entries:
        author = ((e.get("author") or {}).get("name") or {}).get("label")
        content = ((e.get("content") or {}).get("label"))
        title = ((e.get("title") or {}).get("label"))
        rating = ((e.get("im:rating") or {}).get("label"))
        updated = ((e.get("updated") or {}).get("label"))
        rid = ((e.get("id") or {}).get("label"))

        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –æ—Ç–∑—ã–≤ (—Å–ª—É–∂–µ–±–Ω–∞—è –∑–∞–ø–∏—Å—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è) ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if not (author and content and title and rating and updated and rid):
            continue

        version = ((e.get("im:version") or {}).get("label"))

        parsed.append({
            "review_id": rid,
            "author_name": author,
            "title": title,
            "review_text": content,
            "rating": int(rating) if str(rating).isdigit() else None,
            "review_date_raw": updated,
            "version": version,
        })
    return parsed

def parse_iso_date(date_str: str) -> datetime | None:
    # –ü—Ä–∏–≤–æ–¥–∏–º –¥–∞—Ç—É –∫ UTC –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å cutoff
    if not date_str:
        return None
    try:
        dt = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None


# -----------------------------
# RU-—Ñ–∏–ª—å—Ç—Ä –±–µ–∑ langdetect:
# —Å—á–∏—Ç–∞–µ–º –¥–æ–ª—é –∫–∏—Ä–∏–ª–ª–∏—Ü—ã —Å—Ä–µ–¥–∏ –±—É–∫–≤ –∏ –ø–æ—Ä–æ–≥–æ–º —Ä–µ—à–∞–µ–º "—Ä—É—Å—Å–∫–∏–π / –Ω–µ —Ä—É—Å—Å–∫–∏–π"
# -----------------------------
_CYR_RE = re.compile(r"[–ê-–Ø–∞-—è–Å—ë]")
_LETTER_RE = re.compile(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë]")

def ru_score(text: str) -> float:
    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–ª—é –∫–∏—Ä–∏–ª–ª–∏—Ü—ã —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –±—É–∫–≤ (0..1)
    t = (text or "").strip()
    if not t:
        return 0.0
    letters = _LETTER_RE.findall(t)
    if len(letters) < 12:
        # –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–µ —Å—á–∏—Ç–∞–µ–º –Ω–∞–¥—ë–∂–Ω—ã–º
        return 0.0
    cyr = _CYR_RE.findall(t)
    return len(cyr) / max(len(letters), 1)

def is_russian_text(title: str, body: str, threshold: float = 0.55) -> bool:
    # –°–∫–ª–µ–∏–≤–∞–µ–º title + body –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–ª—é –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
    combined = f"{title or ''} {body or ''}".strip()
    return ru_score(combined) >= threshold


# -----------------------------
# –î–µ–¥—É–ø: review_id –∏–ª–∏ fallback hash
# -----------------------------
def normalized_text_for_hash(text: str) -> str:
    t = (text or "").lower().replace("—ë", "–µ")
    t = re.sub(r"\s+", " ", t).strip()
    return t

def make_fallback_dedupe_key(author_name: str, review_date_iso: str, text: str) -> str:
    base = f"{author_name}||{review_date_iso}||{normalized_text_for_hash(text)}"
    return hashlib.sha256(base.encode("utf-8")).hexdigest()


# -----------------------------
# –ê–≤—Ç–æ-—Ç—ç–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–º (rule-based)
# -----------------------------
TOPIC_ORDER = ["onboarding", "streak", "ads", "subscription", "bugs", "motivation"]

def _normalize_for_matching(text: str) -> str:
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: lower + —ë->–µ + —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ + —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤
    t = (text or "").lower().replace("—ë", "–µ")
    t = re.sub(r"[^\w\s]", " ", t, flags=re.UNICODE)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def _compile_keyword_patterns():
    # –°–ª–æ–≤–∞—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤/—Ñ—Ä–∞–∑ –ø–æ —Ç–µ–º–∞–º (EN + RU + ES + PT + FR + DE)
    kw = {
        "onboarding": [
            "onboarding","tutorial","getting started","first lesson","intro lesson","sign up","signup","log in","login","register","registration",
            "–ø–µ—Ä–≤—ã–µ —à–∞–≥–∏","–≤–≤–æ–¥–Ω—ã–π —É—Ä–æ–∫","–æ–±—É—á–µ–Ω–∏–µ","–ø–æ–¥—Å–∫–∞–∑–∫–∏","—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è","–≤–æ–π—Ç–∏","–≤—Ö–æ–¥",
            "tutorial","primeros pasos","registro","iniciar sesion","inicio de sesion",
            "tutorial","primeiros passos","registro","iniciar sessao","login",
            "tutoriel","premiers pas","inscription","connexion",
            "tutorial","erste schritte","registrierung","anmeldung","einloggen",
        ],
        "streak": [
            "streak","daily streak","keep streak","streak freeze",
            "—Å–µ—Ä–∏—è","—Å—Ç—Ä–∏–∫","–∑–∞–º–æ—Ä–æ–∑–∫–∞ —Å–µ—Ä–∏–∏","–¥–Ω–µ–≤–Ω–∞—è —Å–µ—Ä–∏—è",
            "racha","racha diaria","congelar racha",
            "sequencia","sequencia diaria","congelar sequencia",
            "serie","serie quotidienne","geler la serie",
            "serie","tagesserie","serie einfrieren",
        ],
        "ads": [
            # ad/ads ‚Äî —Å—Ç—Ä–æ–≥–æ –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º —Å–ª–æ–≤, —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å "advice"
            "ads","ad","advertising","advertisement","commercials","too many ads","banner ad","video ad","adblock",
            "—Ä–µ–∫–ª–∞–º–∞","–±–∞–Ω–Ω–µ—Ä","—Ä–æ–ª–∏–∫","–≤–∏–¥–µ–æ —Ä–µ–∫–ª–∞–º–∞","—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ä–µ–∫–ª–∞–º—ã","–∞–¥–±–ª–æ–∫","adblock",
            "anuncios","publicidad","demasiados anuncios",
            "anuncios","publicidade","muitos anuncios",
            "publicite","annonces","trop de publicite",
            "werbung","anzeigen","zu viel werbung",
        ],
        "subscription": [
            # –í–∞–∂–Ω–æ: "free trial" –æ—Ç–¥–µ–ª—å–Ω–æ, –Ω–µ –ø—Ä–æ—Å—Ç–æ "free"
            "subscription","subscribe","premium","plus","super","payment","price","billing","trial","free trial","refund","cancel subscription",
            "–ø–æ–¥–ø–∏—Å–∫–∞","–ø–æ–¥–ø–∏—Å","–æ–ø–ª–∞—Ç","–ø–ª–∞—Ç–µ–∂","—Ü–µ–Ω–∞","—Å—Ç–æ–∏–º–æ—Å—Ç","–ø—Ä–æ–±–Ω—ã–π –ø–µ—Ä–∏–æ–¥","—Ç—Ä–∏–∞–ª","–≤–æ–∑–≤—Ä–∞—Ç","–æ—Ç–º–µ–Ω–∞ –ø–æ–¥–ø–∏—Å–∫–∏",
            "suscripcion","suscrib","pago","precio","facturacion","prueba gratis","reembolso","cancelar suscripcion",
            "assinatura","assinar","pagamento","preco","faturamento","teste gratis","reembolso","cancelar assinatura",
            "abonnement","s abonner","paiement","prix","facturation","essai gratuit","remboursement","annuler l abonnement",
            "abo","abonnement","abonnieren","zahlung","preis","abrechnung","kostenloser test","ruckerstattung","abo kundigen",
        ],
        "bugs": [
            "bug","glitch","crash","crashes","freezes","freeze","lag","error","not working","broken",
            "–±–∞–≥","–≥–ª—é–∫","–≤—ã–ª–µ—Ç–∞–µ—Ç","–∫—Ä–∞—à","–∑–∞–≤–∏—Å–∞–µ—Ç","–ª–∞–≥–∞–µ—Ç","–æ—à–∏–±–∫–∞","–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç",
            "error","no funciona","se cierra","bloquea","falla","bug",
            "erro","nao funciona","fecha","trava","falha","bug",
            "bug","erreur","ne marche pas","plante","bloque","ralentit",
            "fehler","funktioniert nicht","sturz","absturz","hangt","ruckelt","bug",
        ],
        "motivation": [
            "motivate","motivation","habit","progress","goals","reminders","fun","addictive","encouraging",
            "–≤–æ–≤–ª–µ–∫–∞–µ—Ç","–º–æ—Ç–∏–≤–∞—Ü–∏—è","–ø—Ä–∏–≤—ã—á–∫–∞","–ø—Ä–æ–≥—Ä–µ—Å—Å","—Ü–µ–ª–∏","–Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è","–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ","–∑–∞—Ç—è–≥–∏–≤–∞–µ—Ç",
            "motiva","motivacion","habito","progreso","metas","recordatorios","divertido","adictivo",
            "motiva","motivacao","habito","progresso","metas","lembretes","divertido","viciante",
            "motivation","motiver","habitude","progres","objectifs","rappels","amusant","addictif",
            "motivation","motiviert","gewohnheit","fortschritt","ziele","erinnerungen","macht spass","suchtig",
        ],
    }

    patterns = {}
    for topic, words in kw.items():
        compiled = []
        for w in words:
            w_norm = _normalize_for_matching(w)
            if not w_norm:
                continue

            is_latin_single = bool(re.fullmatch(r"[a-z0-9]+", w_norm))

            if topic == "ads" and w_norm in ("ad", "ads"):
                pat = re.compile(rf"\b{re.escape(w_norm)}\b", flags=re.IGNORECASE)
            elif is_latin_single:
                pat = re.compile(rf"\b{re.escape(w_norm)}\b", flags=re.IGNORECASE)
            else:
                pat = re.compile(re.escape(w_norm), flags=re.IGNORECASE)

            compiled.append(pat)

        patterns[topic] = compiled

    return patterns

TOPIC_PATTERNS = _compile_keyword_patterns()

def tag_topics(df: pd.DataFrame) -> pd.DataFrame:
    # –î–æ–±–∞–≤–ª—è–µ–º topic_tags –∏ –±—É–ª–µ–≤—ã–µ topic_*
    df = df.copy()

    def match_topics(title: str, text: str) -> dict:
        norm = _normalize_for_matching(f"{title or ''} {text or ''}")
        hits = {t: 0 for t in TOPIC_ORDER}
        for topic in TOPIC_ORDER:
            for p in TOPIC_PATTERNS[topic]:
                if p.search(norm):
                    hits[topic] = 1
                    break
        return hits

    tags_col = []
    topic_cols = {f"topic_{t}": [] for t in TOPIC_ORDER}

    for _, row in df.iterrows():
        hits = match_topics(row.get("title"), row.get("review_text"))
        for t in TOPIC_ORDER:
            topic_cols[f"topic_{t}"].append(hits[t])
        tags_col.append(",".join([t for t in TOPIC_ORDER if hits[t] == 1]))

    df["topic_tags"] = tags_col
    for t in TOPIC_ORDER:
        df[f"topic_{t}"] = topic_cols[f"topic_{t}"]

    return df


# -----------------------------
# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: —Å–±–æ—Ä + RU-only + —Ç—ç–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# -----------------------------
def scrape_appstore_reviews_all_countries(
    app_url: str,
    per_country_limit: int = 50,
    days: int = 7,
    ru_threshold: float = 0.55,
    delay_between_requests_min: float = 0.25,
    delay_between_requests_max: float = 0.55,
    log_box=None,
    progress_callback=None,
):
    app_id = extract_app_id(app_url)
    default_country = extract_default_country_from_url(app_url)

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari",
        "Accept": "application/json,text/plain,*/*",
    })

    app_name = get_app_name(session, app_id, preferred_country=default_country)
    now_utc = datetime.now(timezone.utc)
    cutoff = now_utc - relativedelta(days=days)

    if log_box:
        ui_log(log_box, f"app_id={app_id}, default_country={default_country}, app_name={app_name}")
        ui_log(log_box, f"Cutoff (UTC) = {cutoff.isoformat()} (–ø–æ—Å–ª–µ–¥–Ω–∏–µ {days} –¥–Ω–µ–π)")
        ui_log(log_box, f"RU-—Ñ–∏–ª—å—Ç—Ä: –¥–æ–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã ‚â• {ru_threshold:.2f}")

    all_rows = []
    seen_review_ids = set()
    seen_fallback = set()

    countries = [default_country] + [c for c in STORE_FRONTS if c != default_country]
    total_countries = len(countries)

    for idx, country in enumerate(countries):
        if progress_callback:
            progress_callback((idx + 1) / total_countries, country)

        lookup = itunes_lookup(session, app_id, country)
        if not lookup:
            if log_box:
                ui_log(log_box, f"[{country}] –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ –ø–æ lookup -> –ø—Ä–æ–ø—É—Å–∫")
            continue

        pages = 0
        scanned = 0
        kept_ru = 0
        filtered_old = 0
        stop_due_to_old = False
        page = 1

        time.sleep(random.uniform(delay_between_requests_min, delay_between_requests_max))

        while scanned < per_country_limit and not stop_due_to_old:
            pages += 1
            rss_url = build_rss_url(country, app_id, page)

            r = request_with_retry(session, rss_url)
            time.sleep(random.uniform(delay_between_requests_min, delay_between_requests_max))

            if not r:
                if log_box:
                    ui_log(log_box, f"[{country}] page={page}: –∑–∞–ø—Ä–æ—Å –Ω–µ —É–¥–∞–ª—Å—è -> —Å—Ç–æ–ø –ø–æ —Å—Ç—Ä–∞–Ω–µ")
                break

            try:
                feed_json = r.json()
            except Exception:
                if log_box:
                    ui_log(log_box, f"[{country}] page={page}: –Ω–µ JSON -> —Å—Ç–æ–ø –ø–æ —Å—Ç—Ä–∞–Ω–µ")
                break

            reviews = parse_rss_reviews(feed_json)
            if not reviews:
                if log_box:
                    ui_log(log_box, f"[{country}] page={page}: –æ—Ç–∑—ã–≤–æ–≤ –Ω–µ—Ç -> —Å—Ç–æ–ø –ø–æ —Å—Ç—Ä–∞–Ω–µ")
                break

            for rv in reviews:
                if scanned >= per_country_limit:
                    break

                dt = parse_iso_date(rv.get("review_date_raw"))
                if not dt:
                    continue

                if dt < cutoff:
                    filtered_old += 1
                    stop_due_to_old = True
                    break

                review_id = rv.get("review_id") or ""
                title = rv.get("title") or ""
                text = rv.get("review_text") or ""
                author = rv.get("author_name") or ""
                rating = rv.get("rating")
                version = rv.get("version")
                review_date_iso = dt.isoformat()

                # –î–µ–¥—É–ø
                if review_id:
                    if review_id in seen_review_ids:
                        continue
                else:
                    fb = make_fallback_dedupe_key(author, review_date_iso, f"{title}\n{text}")
                    if fb in seen_fallback:
                        continue
                    seen_fallback.add(fb)

                scanned += 1
                if review_id:
                    seen_review_ids.add(review_id)

                # RU-only —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–æ–ª–µ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
                if not is_russian_text(title, text, threshold=ru_threshold):
                    continue

                kept_ru += 1
                all_rows.append({
                    "app_id": app_id,
                    "app_name": app_name,
                    "country": country,
                    "review_id": review_id if review_id else None,
                    "author_name": author,
                    "rating": rating,
                    "title": title,
                    "review_text": text,
                    "review_date": review_date_iso,
                    "version": version,
                    "language": "ru",
                    "source_url": app_url,
                })

            if log_box:
                ui_log(
                    log_box,
                    f"[{country}] pages={pages}, scanned={scanned}/{per_country_limit}, kept_ru={kept_ru}, filtered_old={filtered_old} (page={page})"
                )

            page += 1

        if log_box:
            ui_log(log_box, f"[{country}] DONE: pages={pages}, scanned={scanned}, kept_ru={kept_ru}")

    df = pd.DataFrame(all_rows)

    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —Å—Ö–µ–º—É
    base_cols = [
        "app_id","app_name","country","review_id","author_name","rating",
        "title","review_text","review_date","version","language","source_url"
    ]
    for c in base_cols:
        if c not in df.columns:
            df[c] = None

    # –¢—ç–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    if len(df) > 0:
        df = tag_topics(df)
    else:
        df["topic_tags"] = ""
        for t in TOPIC_ORDER:
            df[f"topic_{t}"] = 0

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
    final_cols = [
        "app_id",
        "app_name",
        "country",
        "review_id",
        "author_name",
        "rating",
        "title",
        "review_text",
        "review_date",
        "version",
        "language",
        "topic_tags",
        "topic_onboarding",
        "topic_streak",
        "topic_ads",
        "topic_subscription",
        "topic_bugs",
        "topic_motivation",
        "source_url",
    ]
    df = df[final_cols]
    return df


# ===========================================
# Streamlit UI
# ===========================================
st.set_page_config(page_title="App Store Reviews (RU) + Topic Tags", layout="wide")

st.title("App Store –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å—Ç—Ä–∞–Ω—ã) ‚Üí —Ç–æ–ª—å–∫–æ RU ‚Üí —Ç—ç–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–º")
st.caption("–ò—Å—Ç–æ—á–Ω–∏–∫: Apple RSS JSON (customerreviews) + iTunes Lookup. –ë–µ–∑ Selenium/Playwright. –ë–µ–∑ langdetect.")

with st.sidebar:
    st.header("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã")
    app_url = st.text_input(
        "App Store URL",
        value="https://apps.apple.com/us/app/duolingo-language-lessons/id570060128"
    )
    per_country_limit = st.slider("–õ–∏–º–∏—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω—É (—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º)", 5, 50, 50, 5)
    days = st.slider("–ü–µ—Ä–∏–æ–¥ (–¥–Ω–µ–π –Ω–∞–∑–∞–¥)", 1, 30, 7, 1)
    ru_threshold = st.slider("RU-–ø–æ—Ä–æ–≥ (–¥–æ–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã)", 0.30, 0.90, 0.55, 0.05)

    st.divider()
    st.write("–°–∫–æ—Ä–æ—Å—Ç—å (—á—Ç–æ–±—ã –º–µ–Ω—å—à–µ –ª–æ–≤–∏—Ç—å 429):")
    delay_min = st.slider("–ü–∞—É–∑–∞ min (—Å–µ–∫)", 0.0, 2.0, 0.25, 0.05)
    delay_max = st.slider("–ü–∞—É–∑–∞ max (—Å–µ–∫)", 0.0, 3.0, 0.55, 0.05)
    if delay_max < delay_min:
        st.warning("delay_max –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å ‚â• delay_min")

run_btn = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–±–æ—Ä")

log_box = st.empty()
progress_bar = st.progress(0, text="–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞...")
country_badge = st.empty()

def progress_cb(progress_value: float, country: str):
    progress_bar.progress(int(progress_value * 100), text=f"–°–∫–∞–Ω–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω—ã... ({country})")
    country_badge.info(f"–¢–µ–∫—É—â–∞—è —Å—Ç—Ä–∞–Ω–∞: {country}")

if run_btn:
    try:
        df = scrape_appstore_reviews_all_countries(
            app_url=app_url,
            per_country_limit=per_country_limit,
            days=days,
            ru_threshold=ru_threshold,
            delay_between_requests_min=delay_min,
            delay_between_requests_max=delay_max,
            log_box=log_box,
            progress_callback=progress_cb,
        )
        progress_bar.progress(100, text="–ì–æ—Ç–æ–≤–æ ‚úÖ")

        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç")
        st.write(f"–°–æ–±—Ä–∞–Ω–æ RU-–æ—Ç–∑—ã–≤–æ–≤: **{len(df)}**")
        st.dataframe(df, use_container_width=True)

        # –°–≤–æ–¥–∫–∞ –ø–æ —Ç–µ–º–∞–º
        st.subheader("–°–≤–æ–¥–∫–∞ –ø–æ —Ç–µ–º–∞–º")
        if len(df) > 0:
            exploded = df["topic_tags"].str.split(",", expand=False).explode()
            exploded = exploded[exploded.notna() & (exploded != "")]
            if len(exploded) > 0:
                counts = exploded.value_counts().reindex(TOPIC_ORDER).fillna(0).astype(int)
                st.write(counts)
            else:
                st.write("–°–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —Ç–µ–º–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

            st.subheader("–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –ø–æ —Ç–µ–º–∞–º")
            rows = []
            for t in TOPIC_ORDER:
                sub = df[df[f"topic_{t}"] == 1]
                if len(sub) == 0:
                    continue
                avg = sub["rating"].dropna().astype(float).mean()
                rows.append({"topic": t, "avg_rating": round(float(avg), 2), "n": len(sub)})
            if rows:
                st.dataframe(pd.DataFrame(rows), use_container_width=True)
            else:
                st.write("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞.")

        # –°–∫–∞—á–∞—Ç—å CSV
        out_name = f"appstore_reviews_all_countries_{extract_app_id(app_url)}_{datetime.now().strftime('%Y%m%d')}.csv"
        csv_bytes = df.to_csv(index=False, encoding="utf-8", quoting=csv.QUOTE_ALL).encode("utf-8")

        st.download_button(
            label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å CSV",
            data=csv_bytes,
            file_name=out_name,
            mime="text/csv",
        )

    except Exception as e:
        progress_bar.progress(0, text="–û—à–∏–±–∫–∞ ‚ùå")
        st.error(f"–û—à–∏–±–∫–∞: {e}")
